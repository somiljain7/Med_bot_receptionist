{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda\\envs\\GEN_AI_T1\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import models,QdrantClient\n",
    "import numpy\n",
    "#pip install 'numpy<2'\n",
    "import time\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import random\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\") #for a different NER \n",
    "    #model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\") #for a different NER\n",
    "    ref_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    #nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer) #for a different NER\n",
    "    return ref_model,nlp\n",
    "\n",
    "def location_ner(nlp,res):\n",
    "    doc = nlp(res)\n",
    "    # Extract entities recognized as locations (GPE: Geopolitical Entities, LOC: Non-GPE locations)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    if locations:\n",
    "        return ', '.join(locations)\n",
    "    else:\n",
    "        return 0\n",
    "    #\"I didn't understand your response. Could you please provide the area that you are located in right now?\"\n",
    "    \n",
    "def is_semantically_related(ref_model,sentence, reference_sentences, threshold=0.7):\n",
    "    sentence_embedding = ref_model.encode(sentence, convert_to_tensor=True)\n",
    "    for ref_sentence in reference_sentences:\n",
    "        ref_embedding = ref_model.encode(ref_sentence, convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(sentence_embedding, ref_embedding)\n",
    "        if similarity.item() > threshold:\n",
    "            return True, similarity.item()\n",
    "\n",
    "    return False, 0\n",
    "\n",
    "def relevance_check(ref_model,user_sentence): \n",
    "    reference_sentences_set1 = [\"too late\", \"arrive too late\",\"i am panicing\", \"won't make it in time\", \"delayed\", \"take too long\", \"not soon enough\", \"we can't wait\", \"need help immediately\", \"time-sensitive\", \"urgent\" ,\"The doctor will not arrive in time.\", \"I am worried the doctor will be too late.\"]\n",
    "    reference_sentences_set2 = [\"No Thank you\",'Thanks for all the help','Thanks']\n",
    "    is_related_1, similarity_score_1 = is_semantically_related(ref_model,user_sentence, reference_sentences=reference_sentences_set1)\n",
    "    is_related_2, similarity_score_2 = is_semantically_related(ref_model,user_sentence, reference_sentences=reference_sentences_set2)\n",
    "    if is_related_1 or is_related_2:\n",
    "        if similarity_score_1 > similarity_score_2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def vector_search(qdrant,encoder,x,collect_name):\n",
    "    # Search within a Vector database \n",
    "    #args:\n",
    "    #qdrant: object of QdrantClient\n",
    "    #encoder:  encoder for semantic similarity\n",
    "    #x: sentence for calculating semantic similarity with records \n",
    "    hits = qdrant.search(\n",
    "        collection_name=collect_name,\n",
    "        query_vector=encoder.encode(x).tolist(),\n",
    "        limit=1\n",
    "    )\n",
    "    for hit in hits:\n",
    "        return (hit.payload,'score:',hit.score)\n",
    "\n",
    "\n",
    "def process_response(res):\n",
    "    # Generating the Output of vector search\n",
    "    encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    qdrant=vector_database_init()\n",
    "    llm_output = f\"{vector_search(qdrant,encoder,res,\"Responses\")[0][\"name\"]}\"\n",
    "    return llm_output\n",
    "\n",
    "def vector_database_init():\n",
    "    # initializing Qdrant client at port 6333\n",
    "    qdrant = QdrantClient(\n",
    "        host='localhost',\n",
    "        port=6333\n",
    "    )\n",
    "    return qdrant\n",
    "\n",
    "\n",
    "def process(symptoms):\n",
    "    # Generating the Output of vector search\n",
    "    print(\"\\nProcessing the emergency report...\")\n",
    "    time.sleep(15)  # Simulate processing time\n",
    "    encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    qdrant=vector_database_init()\n",
    "    llm_output = f\"The analysis indicates a possible condition based on the symptoms: {vector_search(qdrant,encoder,symptoms,\"Clinical\")}\"\n",
    "    return llm_output\n",
    "\n",
    "\n",
    "def generate_random_eta():\n",
    "    random_seconds = random.randint(60, 7200)\n",
    "    eta = datetime.timedelta(seconds=random_seconds)\n",
    "    total_seconds = int(eta.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    formatted_eta = f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "    return formatted_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda\\envs\\GEN_AI_T1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is RIYA and i work as a receptionist for Dr. Adrin. How can I help you today?\n",
      "\n",
      "RIYA : Please  Send a message or Report an emergency\n",
      "\n",
      "RIYA : You selected 'Report an emergency'.\n",
      "User: I am not able to breathe properly dealing with chest pain\n",
      "\n",
      "Processing the emergency report...\n",
      "RIYA : Please hold just a sec\n",
      "User: Indore MP India\n",
      "RIYA : Location 'Indore MP India' received.  Dr. Adrin will be coming to their location immediately. ETA:00:23:44\n",
      "\n",
      "RIYA :  Processing complete. Here is the analysis from our system:\n",
      "The analysis indicates a possible condition based on the symptoms: ({'name': 'Unconscious', 'remedy': 'If someone is unconscious or unresponsive, follow the ABC principle: Airway, Breathing, and Circulation. \\n               1. Airway: Open their airway if they are not breathing. \\n               2. Breathing: If the airway is clear but the person is still not breathing, provide rescue breathing.\\n               3. Circulation: Perform chest compressions to maintain blood circulation while doing rescue breathing. \\n               If the person is unresponsive, check their pulse. Begin CPR by pushing against the chest and blowing air into their mouth in a constant rhythm. If their heart has stopped, continue chest compressions.'}, 'score:', 0.4441438)\n",
      "User: it says 23 min , it will too late for his arrival\n",
      "RIYA : I understand that you are worried that Dr. Adrin will arrive too late, meanwhile we would suggest that you follow following remedy:The analysis indicates a possible condition based on the symptoms: ({'name': 'Unconscious', 'remedy': 'If someone is unconscious or unresponsive, follow the ABC principle: Airway, Breathing, and Circulation. \\n               1. Airway: Open their airway if they are not breathing. \\n               2. Breathing: If the airway is clear but the person is still not breathing, provide rescue breathing.\\n               3. Circulation: Perform chest compressions to maintain blood circulation while doing rescue breathing. \\n               If the person is unresponsive, check their pulse. Begin CPR by pushing against the chest and blowing air into their mouth in a constant rhythm. If their heart has stopped, continue chest compressions.'}, 'score:', 0.4441438) \n",
      "\n",
      "\n",
      "RIYA : Thank you for using the service. Stay safe!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ref_model,nlp=model_init()\n",
    "\n",
    "print(\"Hello! My name is RIYA and i work as a receptionist for Dr. Adrin. How can I help you today?\")\n",
    "while True:\n",
    "    print(\"\\nRIYA : Please  Send a message or Report an emergency\")\n",
    "    user_choice = input(\"RIYA : Please  Send a message or Report an emergency\")\n",
    "    choice=process_response(user_choice)\n",
    "    if choice==\"Message\":\n",
    "        message = input(\"\\nPlease enter the message you want to send: \")\n",
    "        print( \"RIYA :Thanks for the message, we will forward it to Dr. Adrin\")\n",
    "        break\n",
    "    elif choice == \"Emergency\":\n",
    "        print(\"\\nRIYA : You selected 'Report an emergency'.\")\n",
    "        symptoms = input(\"Please describe the symptoms: \")\n",
    "        print(f\"User: {symptoms}\")\n",
    "        llm_output = []\n",
    "        def process_and_store():\n",
    "            result = process(symptoms)\n",
    "            llm_output.append(result)  # Store the semantic similarity search output in a list\n",
    "        processing_thread = threading.Thread(target=process_and_store)\n",
    "        processing_thread.start()\n",
    "\n",
    "        print(\"RIYA : Please hold just a sec\")    \n",
    "            # Simultaneously ask for location\n",
    "        location = input(\"RIYA : I am checking what you should do immediately, meanwhile, can you tell me which area are you located right now?\")\n",
    "        ETA=generate_random_eta()\n",
    "        print(f\"User: {location}\")\n",
    "        if location_ner(nlp,location)!=0:\n",
    "            print(f\"RIYA : Location '{location}' received.  Dr. Adrin will be coming to their location immediately. ETA:{ETA}\")\n",
    "        else: \n",
    "            location=input(\"I don't understand that and could you please repeat the area you are located right now (in proper format)? \")\n",
    "            print(f\"User: {location}\")\n",
    "            if(location_ner(nlp,location)==0):\n",
    "                print(\"RIYA : I don’t understand that and unable to fetch your location ,could you please try again later .\")\n",
    "                exit()\n",
    "            else:\n",
    "                print(f\"RIYA : Location '{location}' received.  Dr. Adrin will be coming to their location immediately. ETA:{ETA}\")\n",
    "        processing_thread.join()\n",
    "            \n",
    "            # Print the semantic similarity search output after location input\n",
    "        print(\"\\nRIYA :  Processing complete. Here is the analysis from our system:\")\n",
    "        print(llm_output[0])  # Access the first (and only) element in the list\n",
    "        response = input(\"Do you have anything else to ask ?\") \n",
    "        print(f\"User: {response}\")\n",
    "        res_rev=process_response(response)\n",
    "        #res_rev=relevance_check(ref_model,response)\n",
    "        if res_rev==\"Late\":\n",
    "            print(f\"RIYA : I understand that you are worried that Dr. Adrin will arrive too late, meanwhile we would suggest that you follow following remedy:{llm_output[0]} \\n\"  )\n",
    "\n",
    "        else:\n",
    "            print(\"RIYA : I don't understand that but Don’t worry, please follow the steps, Dr. Adrin will be with you shortly\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"\\nRIYA : Invalid Response. Please mention wether you like to send a message or to report an emergency.\")\n",
    "    \n",
    "print(\"\\nRIYA : Thank you for using the service. Stay safe!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEN_AI_T1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
